{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911ff368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b2e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b488f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "model2 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03842fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fc = nn.Linear(in_features=512, out_features=N_CLASSES, bias=True)\n",
    "model2.fc = nn.Linear(in_features=2048, out_features=N_CLASSES, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3a0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img):\n",
    "    img = np.array(img)\n",
    "    width, height, _ = img.shape\n",
    "    \n",
    "    T = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        transforms.CenterCrop(min(width, height)),\n",
    "        transforms.Resize((299, 299))\n",
    "    ])\n",
    "    \n",
    "    return T(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a26c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebaClassificationDataset(Dataset):\n",
    "    def __init__(self, mode='train', class_mapping=None, prev_class_mapping=None):\n",
    "        with open('identity_CelebA.txt') as f:\n",
    "            label_mapping = dict([line.replace('\\n', '').split(' ') for line in f.readlines()])\n",
    "            self.img2class = {key: int(value) for key, value in label_mapping.items()}\n",
    "            \n",
    "        self.train_classes = sorted(list(set([self.img2class[_] for _ in os.listdir('train')])))\n",
    "        self.val_classes   = sorted(list(set([self.img2class[_] for _ in os.listdir('val') if \\\n",
    "                                             self.img2class[_] not in self.train_classes])))\n",
    "        self.test_classes  = sorted(list(set([self.img2class[_] for _ in os.listdir('test') if \\\n",
    "                                  (self.img2class[_] not in self.train_classes)\\\n",
    "                                              and (self.img2class[_] not in self.val_classes)])))\n",
    "        \n",
    "        self.all_classes = self.train_classes + self.val_classes + self.test_classes\n",
    "        \n",
    "        self.class_mapping = {idx_old:idx_new for idx_new, idx_old in enumerate(self.all_classes)}\n",
    "            \n",
    "        self.images = sorted(os.listdir(mode))\n",
    "        self.img2class = {key: self.class_mapping.get(value) for key, value in self.img2class.items()}\n",
    "        self.images = [f'{mode}/{_}' for _ in self.images]\n",
    "        self.img2class = {f'{mode}/{key}': value for key, value in self.img2class.items()}\n",
    "        \n",
    "        if prev_class_mapping:\n",
    "            prev_class_mapping = json.loads(open(prev_class_mapping, 'r').read())\n",
    "            for key, value in prev_class_mapping.items():\n",
    "                if value != N_CLASSES + 1:\n",
    "                    self.img2class[key.replace('train/', f'{mode}/')] = value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx])\n",
    "        img = transform(img)\n",
    "        label = self.img2class[self.images[idx]]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43196b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('img2class.json', 'w') as f:\n",
    "    f.write(json.dumps(train_ds.img2class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e71657",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_ds = CelebaClassificationDataset('train', prev_class_mapping='img2class.json')\n",
    "val_ds   = CelebaClassificationDataset('val'  , prev_class_mapping='img2class.json')\n",
    "test_ds  = CelebaClassificationDataset('test' , prev_class_mapping='img2class.json')\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870cef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load('weights/resnet_49'))\n",
    "model2.load_state_dict(torch.load('weights/inception_49'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3075fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d604684",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('metrics'):\n",
    "    os.mkdir('metrics')\n",
    "\n",
    "def scores(preds_labels):\n",
    "    accuracy  = np.mean([_[0] == _[1] for _ in preds_labels])\n",
    "    fscore    = f1_score([_[1] for _ in preds_labels], [_[0] for _ in preds_labels], average='macro')\n",
    "    conf_mat  = confusion_matrix([_[1] for _ in preds_labels], [_[0] for _ in preds_labels])\n",
    "    precision = precision_score([_[1] for _ in preds_labels], [_[0] for _ in preds_labels], average='macro')\n",
    "    recall    = recall_score([_[1] for _ in preds_labels], [_[0] for _ in preds_labels], average='macro')\n",
    "    \n",
    "    return {'accuracy': accuracy,\n",
    "            'fscore': fscore,\n",
    "            'conf_mat': conf_mat,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "           }\n",
    "\n",
    "def evaluate(model):\n",
    "    model = model.to(device)\n",
    "    preds  = []\n",
    "    golden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images)\n",
    "\n",
    "            preds  += [torch.argmax(_).item() for _ in pred]\n",
    "            golden += [_.item() for _ in labels]\n",
    "    model.train()\n",
    "    \n",
    "    preds_labels = [(pred, label) for pred, label in zip(preds, golden) if label <= N_CLASSES]\n",
    "    metrics = scores(preds_labels)\n",
    "    return metrics\n",
    "\n",
    "def class_train_loop(model, filename, chkpt=None, N_EPOCHS=50):\n",
    "    bad_epoch_counter = 0\n",
    "    running_loss = []\n",
    "    running_metrics = []\n",
    "    last_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'fscore': 0,\n",
    "        'conf_mat': 0,\n",
    "        'precision': 0,\n",
    "        'recall': 0,\n",
    "    }\n",
    "    \n",
    "    if chkpt:\n",
    "        model.load_state_dict(torch.load(chkpt))\n",
    "    \n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        p_bar = tqdm(train_dl)\n",
    "        for images, labels in p_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            preds = model(images)\n",
    "            if type(preds) == models.inception.InceptionOutputs:\n",
    "                loss = criterion(preds.logits, labels)\n",
    "            else:\n",
    "                loss = criterion(preds, labels)\n",
    "            \n",
    "            running_loss += [loss.item()]\n",
    "            p_bar.set_description(f'current_loss: {loss.item()}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        metrics = evaluate(model)\n",
    "        running_metrics += [metrics]\n",
    "        \n",
    "        if metrics['accuracy'] >= max([_['accuracy'] for _ in running_metrics]):\n",
    "            torch.save(model.state_dict(), f'weights/{filename}')\n",
    "            \n",
    "        if metrics['accuracy'] < last_metrics['accuracy']:\n",
    "            bad_epoch_counter += 1\n",
    "        elif (metrics['accuracy'] - last_metrics['accuracy']) < 0.001 * metrics['accuracy']:\n",
    "            bad_epoch_counter += 1\n",
    "        else:\n",
    "            bad_epoch_counter = 0\n",
    "            \n",
    "        if bad_epoch_counter == 5:\n",
    "            with open(f'metrics/{filename}_{epoch}.pickle', 'wb') as f:\n",
    "                pickle.dump({'running_metrics': running_metrics, 'running_loss': running_loss}, f)\n",
    "                \n",
    "            return {'running_metrics': running_metrics, 'running_loss': running_loss}\n",
    "        \n",
    "        last_metrics = metrics\n",
    "        print(f\"{epoch} epoch::{metrics['accuracy']}\")\n",
    "        \n",
    "    with open(f'metrics/{filename}_{epoch}.pickle', 'wb') as f:\n",
    "        pickle.dump({'running_metrics': running_metrics, 'running_loss': running_loss}, f)\n",
    "    return {'running_metrics': running_metrics, 'running_loss': running_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb963b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(model):\n",
    "    model = model.to(device)\n",
    "    preds  = []\n",
    "    golden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images)\n",
    "\n",
    "            preds  += [(torch.argmax(_).item(), torch.max(_).item()) for _ in pred]\n",
    "            golden += [_.item() for _ in labels]\n",
    "    model.train()\n",
    "    \n",
    "    preds_labels = [(pred, label) for pred, label in zip(preds, golden) if (label != N_CLASSES + 1)]\n",
    "    #metrics = scores(preds_labels)\n",
    "    return preds_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57715d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train_loop(model1, 'resnet3', chkpt = 'weights/resnet2', N_EPOCHS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60f48ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train_loop(model2, 'inception3', chkpt = 'weights/inception2', N_EPOCHS=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c7ea762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebaEmbDataset(Dataset):\n",
    "    def __init__(self, mode='train', class_mapping=None, prev_class_mapping=None):\n",
    "        with open('identity_CelebA.txt') as f:\n",
    "            label_mapping = dict([line.replace('\\n', '').split(' ') for line in f.readlines()])\n",
    "            self.img2class = {key: int(value) for key, value in label_mapping.items()}\n",
    "            \n",
    "        self.train_classes = sorted(list(set([self.img2class[_] for _ in os.listdir('train')])))\n",
    "        self.val_classes   = sorted(list(set([self.img2class[_] for _ in os.listdir('val') if \\\n",
    "                                             self.img2class[_] not in self.train_classes])))\n",
    "        self.test_classes  = sorted(list(set([self.img2class[_] for _ in os.listdir('test') if \\\n",
    "                                  (self.img2class[_] not in self.train_classes)\\\n",
    "                                              and (self.img2class[_] not in self.val_classes)])))\n",
    "        \n",
    "        self.all_classes = self.train_classes + self.val_classes + self.test_classes\n",
    "        \n",
    "        self.class_mapping = {idx_old:idx_new for idx_new, idx_old in enumerate(self.all_classes)}\n",
    "            \n",
    "        self.images = sorted(os.listdir(mode))\n",
    "        self.img2class = {key: self.class_mapping.get(value) for key, value in self.img2class.items()}\n",
    "        self.images = [f'{mode}/{_}' for _ in self.images]\n",
    "        self.img2class = {f'{mode}/{key}': value for key, value in self.img2class.items()}\n",
    "        \n",
    "        if prev_class_mapping:\n",
    "            prev_class_mapping = json.loads(open(prev_class_mapping, 'r').read())\n",
    "            for key, value in prev_class_mapping.items():\n",
    "                if value != N_CLASSES + 1:\n",
    "                    self.img2class[key.replace('train/', f'{mode}/')] = value\n",
    "        \n",
    "        \n",
    "        self.dd = defaultdict(list)\n",
    "        for img in self.images:\n",
    "            self.dd[self.img2class[img]] += [img]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor = Image.open(self.images[idx])\n",
    "        anchor = transform(anchor)\n",
    "        label = self.img2class[self.images[idx]]\n",
    "        \n",
    "        positive = np.random.choice(self.dd[label])\n",
    "        positive = transform(Image.open(positive))\n",
    "        \n",
    "        negative = np.random.choice(self.dd[np.random.choice([idx for idx in range(300) if idx != label])])\n",
    "        negative = transform(Image.open(negative))\n",
    "        \n",
    "        return {'anchor': anchor, 'positive': positive, 'negative': negative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bc69df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_emb_ds = CelebaEmbDataset('train', prev_class_mapping='img2class.json')\n",
    "val_emb_ds   = CelebaEmbDataset('val',   prev_class_mapping='img2class.json')\n",
    "test_emb_ds  = CelebaEmbDataset('test',  prev_class_mapping='img2class.json')\n",
    "\n",
    "train_emb_dl = DataLoader(train_emb_ds, batch_size=batch_size, shuffle=True)\n",
    "val_emb_dl   = DataLoader(val_emb_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_emb_dl  = DataLoader(test_emb_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7f815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fc         = nn.Identity(512)\n",
    "model2.fc         = nn.Identity(2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4934afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def calculate_accuracy(val_embs, train_dd, threshold=0.5):\n",
    "    train_dd_temp = deepcopy(train_dd)\n",
    "    \n",
    "    def norm(x):\n",
    "        return x / torch.sqrt(torch.sum(x ** 2))\n",
    "    \n",
    "    cs = nn.CosineSimilarity(dim=1)\n",
    "    \n",
    "    labels = torch.tensor(list(train_dd_temp.keys()))\n",
    "    embs   = torch.vstack(list(train_dd_temp.values()))\n",
    "\n",
    "    final_preds = []\n",
    "\n",
    "    for val_emb, val_face in tqdm(val_embs):\n",
    "        cos_sim = cs(embs, val_emb)\n",
    "        score, label = torch.max(cos_sim).item(), labels[torch.argmax(cos_sim).item()].item()\n",
    "        \n",
    "        if score > threshold:\n",
    "            final_preds += [{'score': score, 'pred': label, 'true': val_face}]\n",
    "        else:\n",
    "            if val_face in train_dd_temp:\n",
    "                final_preds += [{'score': score, 'pred': -1, 'true': val_face}]\n",
    "            else:\n",
    "                train_dd_temp[val_face] = val_emb\n",
    "                embs   = torch.vstack(list(train_dd_temp.values()))\n",
    "                labels = torch.tensor(list(train_dd_temp.keys()))\n",
    "                final_preds += [{'score': score, 'pred': val_face, 'true': val_face}]\n",
    "                \n",
    "    return np.mean([_['pred'] == _['true'] for _ in final_preds]), [(_['pred'], _['true']) for _ in final_preds]\n",
    "\n",
    "def calculate_embs(model, dl):\n",
    "    preds  = []\n",
    "    golden = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images)\n",
    "\n",
    "            preds  += [_.detach() for _ in pred]\n",
    "            golden += [_.item() for _ in labels]\n",
    "    \n",
    "    preds_labels = list(zip(preds, golden))\n",
    "    return preds_labels\n",
    "\n",
    "def evaluate_emb(model, threshold=0.7, mode='val'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    def norm(x):\n",
    "        return x / torch.sqrt(torch.sum(x ** 2))\n",
    "    \n",
    "    train_embs = calculate_embs(model, train_dl)\n",
    "    val_embs   = calculate_embs(model, val_dl)\n",
    "    test_embs  = calculate_embs(model, test_dl)\n",
    "\n",
    "    train_dd = defaultdict(list)\n",
    "    \n",
    "    for emb, face in train_embs:\n",
    "        train_dd[face] += [emb]\n",
    "        \n",
    "    for face, embs in train_dd.items():\n",
    "        train_dd[face] = torch.mean(torch.vstack([norm(emb) for emb in embs]), dim=0)\n",
    "\n",
    "    return train_dd, val_embs, test_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "916327da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses\n",
    "\n",
    "def emb_train_loop(model, filename, chkpt=None, N_EPOCHS=15, lossname='triplet'):\n",
    "    running_loss = []\n",
    "    \n",
    "    def get_preds(preds):\n",
    "        if type(preds) == models.inception.InceptionOutputs:\n",
    "            return preds.logits\n",
    "        else:\n",
    "            return preds\n",
    "    \n",
    "    if chkpt:\n",
    "        model.load_state_dict(torch.load(chkpt))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb_dim = get_preds(model(torch.ones([2, 3, 299, 299]).cuda())).shape[-1]\n",
    "        \n",
    "    if lossname=='triplet':\n",
    "        criterion = nn.TripletMarginLoss()\n",
    "    if lossname=='arcface':\n",
    "        criterion = losses.ArcFaceLoss(N_CLASSES, emb_dim).cuda()\n",
    "    if lossname=='cosface':\n",
    "        criterion = losses.CosFaceLoss(N_CLASSES, emb_dim).cuda()\n",
    "        \n",
    "    opt = torch.optim.Adam(model.parameters(), )\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if lossname == 'triplet':\n",
    "            p_bar = tqdm(train_emb_dl)\n",
    "        else:\n",
    "            p_bar = tqdm(train_dl)\n",
    "            \n",
    "        for batch in p_bar:\n",
    "            if lossname == 'triplet':\n",
    "                for key in batch.keys():\n",
    "                    batch[key] = batch[key].to(device)\n",
    "            else:\n",
    "                batch[0] = batch[0].to(device)\n",
    "                batch[1] = batch[1].to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            if lossname == 'triplet':\n",
    "                preds = {key: get_preds(model(batch[key])) for key in batch.keys()}\n",
    "            else:\n",
    "                preds = model(batch[0])\n",
    "                \n",
    "            if lossname == 'triplet':\n",
    "                loss = criterion(\n",
    "                    preds['anchor'],\n",
    "                    preds['positive'],\n",
    "                    preds['negative']\n",
    "                )\n",
    "            else:\n",
    "                loss = criterion(preds, batch[1])\n",
    "\n",
    "            running_loss += [loss.item()]\n",
    "            p_bar.set_description(f'current_loss: {loss.item()}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        torch.save(model.state_dict(), f'weights/{filename}_{lossname}')\n",
    "        \n",
    "        train_dd, val_embs, test_embs = evaluate_emb(model)\n",
    "        acc, final_preds = calculate_accuracy(val_embs, train_dd, threshold=0.5)\n",
    "        model.train()\n",
    "        with open(f'metrics/{filename}_{lossname}.json', 'w') as f:\n",
    "            f.write(json.dumps({'acc': acc, 'final_preds': final_preds}))\n",
    "        \n",
    "    with open(f'metrics/{filename}_{lossname}.pickle', 'wb') as f:\n",
    "        pickle.dump({'running_loss': running_loss}, f)\n",
    "        \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "\n",
    "idx2model = {0: 'resnet', 0: 'inception'}\n",
    "\n",
    "for loss in ['triplet', 'cosface', 'arcface']:\n",
    "    model1 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
    "    model2 = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "    \n",
    "    model1.fc = nn.Linear(in_features=512, out_features=N_CLASSES, bias=True)\n",
    "    model2.fc = nn.Linear(in_features=2048, out_features=N_CLASSES, bias=True)\n",
    "    \n",
    "    model1.load_state_dict(torch.load('weights/resnet3'))\n",
    "    model2.load_state_dict(torch.load('weights/inception_49'))\n",
    "\n",
    "    model1.fc         = nn.Identity(512)\n",
    "    model2.fc         = nn.Identity(2048)\n",
    "    \n",
    "    for idx, model in enumerate([model1, model1]):\n",
    "        print(idx2model[idx], loss)\n",
    "        emb_train_loop(model, f'{idx2model[idx]}_emb', lossname=loss, N_EPOCHS=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
