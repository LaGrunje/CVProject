{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "911ff368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b2e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d754d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
    "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model.classifier = nn.Linear(768, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3a0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(img):\n",
    "    img = np.array(img)\n",
    "    width, height, _ = img.shape\n",
    "    \n",
    "    T = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        transforms.CenterCrop(min(width, height)),\n",
    "        transforms.Resize((224, 224))\n",
    "    ])\n",
    "    \n",
    "    return T(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a26c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebaClassificationDataset(Dataset):\n",
    "    def __init__(self, mode='train', class_mapping=None, prev_class_mapping=None):\n",
    "        with open('identity_CelebA.txt') as f:\n",
    "            label_mapping = dict([line.replace('\\n', '').split(' ') for line in f.readlines()])\n",
    "            self.img2class = {key: int(value) for key, value in label_mapping.items()}\n",
    "            \n",
    "        self.train_classes = sorted(list(set([self.img2class[_] for _ in os.listdir('train')])))\n",
    "        self.val_classes   = sorted(list(set([self.img2class[_] for _ in os.listdir('val') if \\\n",
    "                                             self.img2class[_] not in self.train_classes])))\n",
    "        self.test_classes  = sorted(list(set([self.img2class[_] for _ in os.listdir('test') if \\\n",
    "                                  (self.img2class[_] not in self.train_classes)\\\n",
    "                                              and (self.img2class[_] not in self.val_classes)])))\n",
    "        \n",
    "        self.all_classes = self.train_classes + self.val_classes + self.test_classes\n",
    "        \n",
    "        self.class_mapping = {idx_old:idx_new for idx_new, idx_old in enumerate(self.all_classes)}\n",
    "            \n",
    "        self.images = sorted(os.listdir(mode))\n",
    "        self.img2class = {key: self.class_mapping.get(value) for key, value in self.img2class.items()}\n",
    "        self.images = [f'{mode}/{_}' for _ in self.images]\n",
    "        self.img2class = {f'{mode}/{key}': value for key, value in self.img2class.items()}\n",
    "        \n",
    "        if prev_class_mapping:\n",
    "            prev_class_mapping = json.loads(open(prev_class_mapping, 'r').read())\n",
    "            for key, value in prev_class_mapping.items():\n",
    "                if value != N_CLASSES + 1:\n",
    "                    self.img2class[key.replace('train/', f'{mode}/')] = value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx])\n",
    "        img = transform(img)\n",
    "        label = self.img2class[self.images[idx]]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174b01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('img2class.json', 'w') as f:\n",
    "    f.write(json.dumps(train_ds.img2class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63e71657",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_ds = CelebaClassificationDataset('train', prev_class_mapping='img2class.json')\n",
    "val_ds   = CelebaClassificationDataset('val'  , prev_class_mapping='img2class.json')\n",
    "test_ds  = CelebaClassificationDataset('test' , prev_class_mapping='img2class.json')\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3075fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d604684",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('metrics'):\n",
    "    os.mkdir('metrics')\n",
    "\n",
    "def scores(preds_labels):\n",
    "    accuracy  = np.mean([_[0] == _[1] for _ in preds_labels])\n",
    "    fscore    = f1_score([_[1] for _ in preds_labels], [_[0] for _ in preds_labels], average='macro')\n",
    "    conf_mat  = confusion_matrix([_[1] for _ in preds_labels], [_[0] for _ in preds_labels])\n",
    "    precision = precision_score([_[1] for _ in preds_labels], [_[0] for _ in preds_labels], average='macro')\n",
    "    recall    = recall_score([_[1] for _ in preds_labels], [_[0] for _ in preds_labels], average='macro')\n",
    "    \n",
    "    return {'accuracy': accuracy,\n",
    "            'fscore': fscore,\n",
    "            'conf_mat': conf_mat,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "           }\n",
    "\n",
    "def evaluate(model):\n",
    "    model = model.to(device)\n",
    "    preds  = []\n",
    "    golden = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images).logits\n",
    "\n",
    "            preds  += [torch.argmax(_).item() for _ in pred]\n",
    "            golden += [_.item() for _ in labels]\n",
    "    model.train()\n",
    "    \n",
    "    preds_labels = [(pred, label) for pred, label in zip(preds, golden) if label != N_CLASSES + 1]\n",
    "    metrics = scores(preds_labels)\n",
    "    return metrics\n",
    "\n",
    "def class_train_loop(model, filename, chkpt=None, N_EPOCHS=100):\n",
    "    bad_epoch_counter = 0\n",
    "    running_loss = []\n",
    "    running_metrics = []\n",
    "    last_metrics = {\n",
    "        'accuracy': 0,\n",
    "        'fscore': 0,\n",
    "        'conf_mat': 0,\n",
    "        'precision': 0,\n",
    "        'recall': 0,\n",
    "    }\n",
    "    \n",
    "    if chkpt:\n",
    "        model.load_state_dict(torch.load(chkpt))\n",
    "    \n",
    "    model = model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        p_bar = tqdm(train_dl)\n",
    "        for images, labels in p_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            preds = model(images)\n",
    "            \n",
    "            loss = criterion(preds.logits, labels)\n",
    "            \n",
    "            running_loss += [loss.item()]\n",
    "            p_bar.set_description(f'current_loss: {loss.item()}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        metrics = evaluate(model)\n",
    "        running_metrics += [metrics]\n",
    "        \n",
    "        if metrics['accuracy'] >= max([_['accuracy'] for _ in running_metrics]):\n",
    "            torch.save(model.state_dict(), f'weights/{filename}')\n",
    "            \n",
    "        if metrics['accuracy'] < last_metrics['accuracy']:\n",
    "            bad_epoch_counter += 1\n",
    "        elif (metrics['accuracy'] - last_metrics['accuracy']) < 0.001 * metrics['accuracy']:\n",
    "            bad_epoch_counter += 1\n",
    "        else:\n",
    "            bad_epoch_counter = 0\n",
    "            \n",
    "        if bad_epoch_counter == 5:\n",
    "            with open(f'metrics/{filename}_{epoch}.pickle', 'wb') as f:\n",
    "                pickle.dump({'running_metrics': running_metrics, 'running_loss': running_loss}, f)\n",
    "                \n",
    "            return {'running_metrics': running_metrics, 'running_loss': running_loss}\n",
    "        \n",
    "        last_metrics = metrics\n",
    "        print(f\"{epoch} epoch::{metrics['accuracy']}\")\n",
    "        \n",
    "    with open(f'metrics/{filename}_{epoch}.pickle', 'wb') as f:\n",
    "        pickle.dump({'running_metrics': running_metrics, 'running_loss': running_loss}, f)\n",
    "    return {'running_metrics': running_metrics, 'running_loss': running_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e852a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_train_loop(model, 'vit', N_EPOCHS=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ea762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebaEmbDataset(Dataset):\n",
    "    def __init__(self, mode='train', class_mapping=None, prev_class_mapping=None):\n",
    "        with open('identity_CelebA.txt') as f:\n",
    "            label_mapping = dict([line.replace('\\n', '').split(' ') for line in f.readlines()])\n",
    "            self.img2class = {key: int(value) for key, value in label_mapping.items()}\n",
    "            \n",
    "        self.train_classes = sorted(list(set([self.img2class[_] for _ in os.listdir('train')])))\n",
    "        self.val_classes   = sorted(list(set([self.img2class[_] for _ in os.listdir('val') if \\\n",
    "                                             self.img2class[_] not in self.train_classes])))\n",
    "        self.test_classes  = sorted(list(set([self.img2class[_] for _ in os.listdir('test') if \\\n",
    "                                  (self.img2class[_] not in self.train_classes)\\\n",
    "                                              and (self.img2class[_] not in self.val_classes)])))\n",
    "        \n",
    "        self.all_classes = self.train_classes + self.val_classes + self.test_classes\n",
    "        \n",
    "        self.class_mapping = {idx_old:idx_new for idx_new, idx_old in enumerate(self.all_classes)}\n",
    "            \n",
    "        self.images = sorted(os.listdir(mode))\n",
    "        self.img2class = {key: self.class_mapping.get(value) for key, value in self.img2class.items()}\n",
    "        self.images = [f'{mode}/{_}' for _ in self.images]\n",
    "        self.img2class = {f'{mode}/{key}': value for key, value in self.img2class.items()}\n",
    "        \n",
    "        if prev_class_mapping:\n",
    "            prev_class_mapping = json.loads(open(prev_class_mapping, 'r').read())\n",
    "            for key, value in prev_class_mapping.items():\n",
    "                if value != N_CLASSES + 1:\n",
    "                    self.img2class[key.replace('train/', f'{mode}/')] = value\n",
    "        \n",
    "        \n",
    "        self.dd = defaultdict(list)\n",
    "        for img in self.images:\n",
    "            self.dd[self.img2class[img]] += [img]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor = Image.open(self.images[idx])\n",
    "        anchor = transform(anchor)\n",
    "        label = self.img2class[self.images[idx]]\n",
    "        \n",
    "        positive = np.random.choice(self.dd[label])\n",
    "        positive = transform(Image.open(positive))\n",
    "        \n",
    "        negative = np.random.choice(self.dd[np.random.choice([idx for idx in range(300) if idx != label])])\n",
    "        negative = transform(Image.open(negative))\n",
    "        \n",
    "        return {'anchor': anchor, 'positive': positive, 'negative': negative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc69df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_emb_ds = CelebaEmbDataset('train', prev_class_mapping='img2class.json')\n",
    "val_emb_ds   = CelebaEmbDataset('val',   prev_class_mapping='img2class.json')\n",
    "test_emb_ds  = CelebaEmbDataset('test',  prev_class_mapping='img2class.json')\n",
    "\n",
    "train_emb_dl = DataLoader(train_emb_ds, batch_size=batch_size, shuffle=True)\n",
    "val_emb_dl   = DataLoader(val_emb_ds,   batch_size=batch_size, shuffle=False)\n",
    "test_emb_dl  = DataLoader(test_emb_ds,  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Identity(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916327da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning import losses\n",
    "\n",
    "def emb_train_loop(model, filename, chkpt=None, N_EPOCHS=15, lossname='triplet'):\n",
    "    running_loss = []\n",
    "    \n",
    "    def get_preds(preds):\n",
    "        if type(preds) == models.inception.InceptionOutputs:\n",
    "            return preds.logits\n",
    "        else:\n",
    "            return preds\n",
    "    \n",
    "    if chkpt:\n",
    "        model.load_state_dict(torch.load(chkpt))\n",
    "    \n",
    "    model = model.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb_dim = model(torch.ones([1, 3, 224, 224]).cuda()).logits.shape[-1]\n",
    "        \n",
    "    if lossname=='triplet':\n",
    "        criterion = nn.TripletMarginLoss()\n",
    "    if lossname=='arcface':\n",
    "        criterion = losses.ArcFaceLoss(N_CLASSES, emb_dim).cuda()\n",
    "    if lossname=='cosface':\n",
    "        criterion = losses.CosFaceLoss(N_CLASSES, emb_dim).cuda()\n",
    "        \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "    \n",
    "    for epoch in range(N_EPOCHS):\n",
    "        if lossname == 'triplet':\n",
    "            p_bar = tqdm(train_emb_dl)\n",
    "        else:\n",
    "            p_bar = tqdm(train_dl)\n",
    "            \n",
    "        for batch in p_bar:\n",
    "            if lossname == 'triplet':\n",
    "                for key in batch.keys():\n",
    "                    batch[key] = batch[key].to(device)\n",
    "            else:\n",
    "                batch[0] = batch[0].to(device)\n",
    "                batch[1] = batch[1].to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "            if lossname == 'triplet':\n",
    "                preds = {key: get_preds(model(batch[key])) for key in batch.keys()}\n",
    "            else:\n",
    "                preds = model(batch[0])\n",
    "                \n",
    "            if lossname == 'triplet':\n",
    "                loss = criterion(\n",
    "                    preds['anchor'].logits,\n",
    "                    preds['positive'].logits,\n",
    "                    preds['negative'].logits\n",
    "                )\n",
    "            else:\n",
    "                loss = criterion(preds, batch[1])\n",
    "\n",
    "            running_loss += [loss.item()]\n",
    "            p_bar.set_description(f'current_loss: {loss.item()}')\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        torch.save(model.state_dict(), f'weights/{filename}_{lossname}')\n",
    "        \n",
    "    with open(f'metrics/{filename}_{lossname}.pickle', 'wb') as f:\n",
    "        pickle.dump({'running_loss': running_loss}, f)\n",
    "        \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embs(model, dl):\n",
    "    preds  = []\n",
    "    golden = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dl):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images).logits\n",
    "\n",
    "            preds  += [_.detach() for _ in pred]\n",
    "            golden += [_.item() for _ in labels]\n",
    "    \n",
    "    preds_labels = list(zip(preds, golden))\n",
    "    return preds_labels\n",
    "\n",
    "def calculate_accuracy(val_embs, train_dd, threshold):\n",
    "    def norm(x):\n",
    "        return x / torch.sqrt(torch.sum(x ** 2))\n",
    "    \n",
    "    cs = nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    val_preds_real = []\n",
    "    all_scores = []\n",
    "\n",
    "    for val_emb, val_face in tqdm(val_embs):\n",
    "        face_score = []\n",
    "        for face, emb in train_dd.items():\n",
    "            face_score += [(face, cs(val_emb, emb))]\n",
    "\n",
    "        face_score = sorted(face_score, key = lambda x: -x[1])\n",
    "\n",
    "        if face_score[0][1] > threshold:\n",
    "            val_preds_real += [[face_score[0][0], val_face]]\n",
    "            all_scores += [{'score': face_score[0][1], 'pred': face_score[0][0], 'true': val_face}]\n",
    "        else:\n",
    "            if val_face in train_dd:\n",
    "                val_preds_real += [[-1, val_face]]\n",
    "                all_scores += [{'score': face_score[0][1], 'pred': -1, 'true': val_face}]\n",
    "            else:\n",
    "                train_dd[val_face] = val_emb\n",
    "                val_preds_real += [[val_face, val_face]]\n",
    "                all_scores += [{'score': face_score[0][1], 'pred': val_face, 'true': val_face}]\n",
    "                \n",
    "    return np.mean([_[0] == _[1] for _ in val_preds_real])\n",
    "\n",
    "def evaluate_emb(model, threshold=0.7, mode='val'):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    def norm(x):\n",
    "        return x / torch.sqrt(torch.sum(x ** 2))\n",
    "    \n",
    "    train_embs = calculate_embs(model, train_dl)\n",
    "    val_embs   = calculate_embs(model, val_dl)\n",
    "    test_embs  = calculate_embs(model, test_dl)\n",
    "\n",
    "    train_dd = defaultdict(list)\n",
    "    \n",
    "    for emb, face in train_embs:\n",
    "        train_dd[face] += [emb]\n",
    "        \n",
    "    for face, embs in train_dd.items():\n",
    "        train_dd[face] = torch.mean(torch.vstack([norm(emb) for emb in embs]), dim=0)\n",
    "        \n",
    "    if mode == 'val':\n",
    "        result = calculate_accuracy(val_embs, train_dd, threshold)\n",
    "    else:\n",
    "        result = calculate_accuracy(test_embs, train_dd, threshold)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "\n",
    "idx2model = {0: 'vit'}\n",
    "\n",
    "for loss in ['triplet', 'cosface', 'arcface']:\n",
    "    model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "    model.classifier = nn.Linear(768, N_CLASSES)\n",
    "    model.load_state_dict(torch.load('weights/vit'))\n",
    "    model.classifier = nn.Identity(768)\n",
    "    \n",
    "    for idx, model in enumerate([model]):\n",
    "        emb_train_loop(model, f'{idx2model[idx]}_emb', lossname=loss, N_EPOCHS=15)\n",
    "        \n",
    "        thr_score = []\n",
    "#         for thr in [0.65, 0.68, 0.7, 0.72, 0.75, 0.78, 0.8, 0.82, 0.85]:\n",
    "        for thr in [0.65, 0.7, 0.75, 0.8, 0.85]:\n",
    "            res = [thr, evaluate_emb(model, threshold=thr, mode='val')]\n",
    "            thr_score += [res]\n",
    "            string = f\"thr: {res[0]}, {res[1]}, mode==val\"\n",
    "            print(string)\n",
    "            \n",
    "        best_thr = sorted(thr_score, key=lambda x: -x[1])[0][0]\n",
    "        test_score = evaluate_emb(model, threshold=0.7, mode='val')\n",
    "        \n",
    "        stats += [{'test_score': test_score, \n",
    "                   'best_thr': best_thr, \n",
    "                   'val_scores': res, \n",
    "                   'lossname': loss, \n",
    "                   'model': idx2model[idx]}]\n",
    "        \n",
    "        print(stats)\n",
    "            \n",
    "        with open('trans_stats.json', 'w') as f:\n",
    "            f.write(json.dumps(stats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
